I"Í<h1 id="introduction">Introduction</h1>

<h1 id="chap-1-introduction">Chap 1: Introduction</h1>

<hr />

<p>The concepts in these notes are from the <em>Introduction to Linear Algebra</em> textbook by Gilbert Strang. Iâ€™ll also be taking intuitions developed by Grant Sanderson in his <em>Linear Algebra</em> video series. These intuitions are particularly important and useful in Robotics. The recipes and algorithms Iâ€™ll leave for the textbook, and refer you to the corresponding sections.</p>

<h1 id="what-are-vectors">What are Vectors?</h1>

<hr />

<h2 id="vectors-and-linear-combination">Vectors and Linear Combination</h2>

<p>Vectors are a collection of numbers, represented as a column. They usually represent a set of physical quantities, coordinates of a frame of reference in robotics(Physics), or stock prices for different companies(Data Science). In general, a vector is a form of abstraction in this regard, and we are trying to dig out general principles that can apply all over. In Linear Algebra, the most atomic entity is the column vector</p>

\[\boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{bmatrix} \,.\]

<h3 id="basic-operations-in-vectors">Basic Operations in vectors</h3>

<p>The basis (not to be confused with basis vector) for Linear Algebra are the following two operations</p>

<ul>
  <li>Vector addition</li>
  <li>Scalar multiplication</li>
</ul>

<aside>
ðŸ’¡ Vector Addition takes the form of adding the two constituents numbers $x_{1}$ and $x_{2}$ of the vector $\textbf{x}$ and the numbers $y_{1}$ and $y_{2}$ of the vector $\textbf{y}$.

</aside>

<h3 id="linear-combination">Linear Combination</h3>

<p>A Linear Combination is the combination of a vector multiplied by a scalar, added to another vector multiplied with another scalar.  If $v_{1},â€¦,v_{n}$ are column vectors (as described above) and $a_{1},â€¦,a_{n}$ are scalars. A <em>Linear Combination</em> is defined as the following</p>

\[a_1 \mathbf v_1 + a_2 \mathbf v_2 + a_3 \mathbf v_3 + \cdots + a_n \mathbf v_n.\]

<p>This operation is the most crucial operation in Linear Algebra. It specifies the <em>Linear Combination</em> of two or more column vectors. This also happens to be the result of a matrix-vector multiplication.</p>

<p>How do you visualise these vectors and the operation of a linear combination?</p>

<p>A vector can be visualised in one of 3 ways</p>

<ul>
  <li>A set of $n$ numbers, where $n$ is the dimension of the vector.</li>
  <li>An arrow from $(0, 0)$</li>
  <li>A point in the $n$-dimensional plane.</li>
</ul>

<p>And a linear combination of a bunch of vectors results in a vector that is visualised in the same way.</p>

<h3 id="important-questions">Important questions</h3>

<p>The subsequent set of questions asked is for the case of vectors in 3 dimensions.</p>

<p>What do all Linear Combinations of a single vector $c\textbf{u}$ look like?</p>

<p>The combinations fill a line through $(0, 0, 0)$.</p>

<p>What do all Linear Combinations of 2 vectors, $c\textbf{u} + d\textbf{v}$, look like?</p>

<p>Ideally the combination fill a plane through $(0, 0, 0)$. However, this depends upon if $\textbf{v}$ is linearly dependent upon $\textbf{u}$, or not.</p>

<p>What do all Linear Combinations of 3 vectors, $c\textbf{u} + d\textbf{v} + e\textbf{w}$, look like?</p>

<p>Again, ideally fills a three dimensional space through $(0, 0, 0)$. Same caveat between three vectors.</p>

<h2 id="vector-lengths-and-dot-products">Vector Lengths and Dot Products</h2>

<h3 id="dot-product">Dot Product</h3>

<h3 id="lengths-and-unit-vectors">Lengths and Unit Vectors</h3>

<h3 id="angle-between-two-vectors">Angle between two vectors</h3>

<h3 id="cosine-formulas-and-basic-inequalities">Cosine Formulas and basic inequalities</h3>

<h2 id="matrices">Matrices</h2>

<p>Matrices need to be thought of as a group of column vectors. This is a fundamental change in viewpoint.</p>

<h3 id="linear-equations">Linear Equations</h3>

<h3 id="inverse-matrix">Inverse Matrix</h3>

<h3 id="cyclic-differences">Cyclic Differences</h3>

<h3 id="independence-and-dependence">Independence and Dependence</h3>

<p>Consider the vectors $\textbf{u}, \textbf{v}, \textbf{w}^{*}$. While defining dependence and independence, what is important is the relation between these vectors.</p>

<p>if $\textbf{w}^{*}$ is in the plane of $\textbf{u}$ and $\textbf{v}$ - dependence</p>

<p>if $\textbf{w}^{*}$ is not on the plane of $\textbf{u}$ and $\textbf{v}$ - independence</p>

<p>This has crucial implications when solving linear equations of the type $\textbf{A}\textbf{x} = \textbf{b}$. The columns of $\textbf{A}$ being dependent/independent determines if the system has one solution, many solutions or no solutions. More on this in Chap 3.</p>

<p>Independent columns - $\textbf{A}\textbf{x} = \textbf{0}$ has one solution - $\textbf{A}$ is said to be <strong><em>Invertible</em></strong></p>

<p>Dependent columns - $\textbf{C}\textbf{x} = \textbf{0}$ has many solutions - $\textbf{C}$ is said to be <strong><em>Singular</em></strong></p>
:ET